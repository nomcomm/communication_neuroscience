{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDecoding with ANOVA + SVM: face vs house in the Haxby dataset\n===============================================================\n\nThis example does a simple but efficient decoding on the Haxby dataset:\nusing a feature selection, followed by an SVM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve the files of the Haxby dataset\n----------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\n\n# By default 2nd subject will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('Mask nifti image (3D) is located at: %s' % haxby_dataset.mask)\nprint('Functional nifti image (4D) is located at: %s' %\n      haxby_dataset.func[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the behavioral data\n-------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\n# Load target information as string and give a numerical identifier to each\nbehavioral = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\nconditions = behavioral['labels']\n\n# Restrict the analysis to faces and places\ncondition_mask = behavioral['labels'].isin(['face', 'house'])\nconditions = conditions[condition_mask]\n\n# Confirm that we now have 2 conditions\nprint(conditions.unique())\n\n# Record these as an array of sessions, with fields\n# for condition (face or house) and run\nsession = behavioral[condition_mask].to_records(index=False)\nprint(session.dtype.names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the fMRI data: smooth and apply the mask\n-------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.input_data import NiftiMasker\n\nmask_filename = haxby_dataset.mask\n\n# For decoding, standardizing is often very important\n# note that we are also smoothing the data\nmasker = NiftiMasker(mask_img=mask_filename, smoothing_fwhm=4,\n                     standardize=True, memory=\"nilearn_cache\", memory_level=1)\nfunc_filename = haxby_dataset.func[0]\nX = masker.fit_transform(func_filename)\n# Apply our condition_mask\nX = X[condition_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the decoder\n------------------\nDefine the prediction function to be used.\nHere we use a Support Vector Classification, with a linear kernel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\nsvc = SVC(kernel='linear')\n\n# Define the dimension reduction to be used.\n# Here we use a classical univariate feature selection based on F-test,\n# namely Anova. When doing full-brain analysis, it is better to use\n# SelectPercentile, keeping 5% of voxels\n# (because it is independent of the resolution of the data).\nfrom sklearn.feature_selection import SelectPercentile, f_classif\nfeature_selection = SelectPercentile(f_classif, percentile=5)\n\n# We have our classifier (SVC), our feature selection (SelectPercentile),and now,\n# we can plug them together in a *pipeline* that performs the two operations\n# successively:\nfrom sklearn.pipeline import Pipeline\nanova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the decoder and predict\n----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "anova_svc.fit(X, conditions)\ny_pred = anova_svc.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain prediction scores via cross validation\n-----------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n\n# Define the cross-validation scheme used for validation.\n# Here we use a LeaveOneGroupOut cross-validation on the session group\n# which corresponds to a leave-one-session-out\ncv = LeaveOneGroupOut()\n\n# Compute the prediction accuracy for the different folds (i.e. session)\ncv_scores = cross_val_score(anova_svc, X, conditions, cv=cv, groups=session)\n\n# Return the corresponding mean prediction accuracy\nclassification_accuracy = cv_scores.mean()\n\n# Print the results\nprint(\"Classification accuracy: %.4f / Chance level: %f\" %\n      (classification_accuracy, 1. / len(conditions.unique())))\n# Classification accuracy:  0.70370 / Chance level: 0.5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the results\n----------------------\nLook at the SVC's discriminating weights\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coef = svc.coef_\n# reverse feature selection\ncoef = feature_selection.inverse_transform(coef)\n# reverse masking\nweight_img = masker.inverse_transform(coef)\n\n\n# Use the mean image as a background to avoid relying on anatomical data\nfrom nilearn import image\nmean_img = image.mean_img(func_filename)\n\n# Create the figure\nfrom nilearn.plotting import plot_stat_map, show\nplot_stat_map(weight_img, mean_img, title='SVM weights')\n\n# Saving the results as a Nifti file may also be important\nweight_img.to_filename('haxby_face_vs_house.nii')\n\n\nshow()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}